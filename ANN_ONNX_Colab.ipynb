{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbenedicto99/TrilhaMLOps/blob/main/ANN_ONNX_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f140806",
      "metadata": {
        "id": "6f140806"
      },
      "source": [
        "\n",
        "# üîß RNA em PyTorch + Exporta√ß√£o/Infer√™ncia ONNX ‚Äî Notebook Colab\n",
        "Este notebook re√∫ne e organiza os scripts fornecidos (`data_utils.py`, `model.py`, `train.py`, `export_onnx.py`, `infer.py`, `infer_onnx.py`) para rodar no Google Colab, com etapas claras de **setup**, **treino**, **exporta√ß√£o para ONNX** e **infer√™ncia (PyTorch e ONNX)**.\n",
        "\n",
        "> Dica: execute as c√©lulas na ordem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66e304a",
      "metadata": {
        "id": "d66e304a"
      },
      "source": [
        "## 1) Ambiente e depend√™ncias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd5b43f",
      "metadata": {
        "id": "6bd5b43f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Verificar vers√£o do Python e GPU\n",
        "import sys, platform, torch\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Plataforma:\", platform.platform())\n",
        "print(\"CUDA dispon√≠vel:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1719a2e",
      "metadata": {
        "id": "f1719a2e"
      },
      "outputs": [],
      "source": [
        "# Instalar depend√™ncias (ajuste conforme necess√°rio)\n",
        "# Obs.: Colab j√° traz muitas libs; caso falte algo do seu projeto, inclua aqui.\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install pytorch-lightning onnx onnxruntime onnxsim\n",
        "# onnxruntime-gpu √© opcional; ativa se houver GPU compat√≠vel\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        !pip -q install onnxruntime-gpu\n",
        "except Exception as e:\n",
        "    print(\"Aviso: onnxruntime-gpu n√£o instalado:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ee010d9",
      "metadata": {
        "id": "9ee010d9"
      },
      "source": [
        "## 2) Estrutura do projeto e caminhos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc1f96b",
      "metadata": {
        "id": "1cc1f96b"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "MODELS_DIR = ROOT / \"models\"\n",
        "ONNX_DIR = ROOT / \"onnx\"\n",
        "\n",
        "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
        "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "ONNX_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"MODELS_DIR:\", MODELS_DIR)\n",
        "print(\"ONNX_DIR:\", ONNX_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6f4a8f",
      "metadata": {
        "id": "3c6f4a8f"
      },
      "source": [
        "### (Opcional) Montar Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21957e35",
      "metadata": {
        "id": "21957e35"
      },
      "outputs": [],
      "source": [
        "USE_GDRIVE = False  # mude para True se quiser salvar no seu Drive\n",
        "if USE_GDRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Ajuste os diret√≥rios, se desejar salvar no Drive:\n",
        "    # ROOT = Path('/content/drive/MyDrive/seu_projeto')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbea5957",
      "metadata": {
        "id": "bbea5957"
      },
      "source": [
        "## 3) C√≥digo-fonte ‚Äî m√≥dulos do projeto\n",
        "Abaixo est√£o os conte√∫dos dos arquivos originais gravados como m√≥dulos locais para facilitar os imports."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98389b01",
      "metadata": {
        "id": "98389b01"
      },
      "source": [
        "### `data_utils.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288d11ba",
      "metadata": {
        "id": "288d11ba"
      },
      "outputs": [],
      "source": [
        "%%writefile data_utils.py\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_dataloaders(model_name: str, dataset_name: str = \"ag_news\", max_length: int = 128, batch_size: int = 16):\n",
        "    ds = load_dataset(dataset_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch[\"text\"], truncation=True, max_length=max_length)\n",
        "\n",
        "    tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "    tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "    tokenized.set_format(type=\"torch\")\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    train_dl = DataLoader(tokenized[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
        "    test_dl = DataLoader(tokenized[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    # Criar valida√ß√£o simples a partir do train (pequeno split)\n",
        "    val_size = min(4000, len(tokenized[\"train\"]))\n",
        "    val_subset = torch.utils.data.Subset(tokenized[\"train\"], range(val_size))\n",
        "    val_dl = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    return train_dl, val_dl, test_dl, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e956d9f3",
      "metadata": {
        "id": "e956d9f3"
      },
      "source": [
        "### `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147a9453",
      "metadata": {
        "id": "147a9453"
      },
      "outputs": [],
      "source": [
        "%%writefile model.py\n",
        "from typing import Any, Dict\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
        "\n",
        "class TextClassifier(pl.LightningModule):\n",
        "    def __init__(self, model_name: str, num_labels: int = 4, lr: float = 5e-5, weight_decay: float = 0.01):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "        self.acc = MulticlassAccuracy(num_classes=num_labels, average=\"macro\")\n",
        "        self.f1 = MulticlassF1Score(num_classes=num_labels, average=\"macro\")\n",
        "\n",
        "    def forward(self, **batch):\n",
        "        return self.model(**batch)\n",
        "\n",
        "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
        "        out = self(**batch)\n",
        "        preds = out.logits.argmax(dim=-1)\n",
        "        acc = self.acc(preds, batch[\"labels\"])\n",
        "        f1 = self.f1(preds, batch[\"labels\"])\n",
        "        self.log_dict({\"train_loss\": out.loss, \"train_acc\": acc, \"train_f1\": f1}, prog_bar=True, on_step=True, on_epoch=True)\n",
        "        return out.loss\n",
        "\n",
        "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
        "        out = self(**batch)\n",
        "        preds = out.logits.argmax(dim=-1)\n",
        "        acc = self.acc(preds, batch[\"labels\"])\n",
        "        f1 = self.f1(preds, batch[\"labels\"])\n",
        "        self.log_dict({\"val_loss\": out.loss, \"val_acc\": acc, \"val_f1\": f1}, prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
        "        out = self(**batch)\n",
        "        preds = out.logits.argmax(dim=-1)\n",
        "        acc = self.acc(preds, batch[\"labels\"])\n",
        "        f1 = self.f1(preds, batch[\"labels\"])\n",
        "        self.log_dict({\"test_loss\": out.loss, \"test_acc\": acc, \"test_f1\": f1}, prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
        "\n",
        "    def on_train_end(self) -> None:\n",
        "        # Salva pesos HF finetunados para export ONNX posterior\n",
        "        self.model.save_pretrained(\"artifacts/hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff4584ec",
      "metadata": {
        "id": "ff4584ec"
      },
      "source": [
        "### `train.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "731349ae",
      "metadata": {
        "id": "731349ae"
      },
      "outputs": [],
      "source": [
        "%%writefile train.py\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import hydra\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "\n",
        "from src.data_utils import get_dataloaders\n",
        "from src.model import TextClassifier\n",
        "\n",
        "def maybe_wandb_logger(cfg):\n",
        "    use_wb = bool(cfg.logging.get(\"use_wandb\", False))\n",
        "    if use_wb and os.environ.get(\"WANDB_API_KEY\"):\n",
        "        import wandb\n",
        "        from pytorch_lightning.loggers import WandbLogger\n",
        "        wandb.login()\n",
        "        return WandbLogger(project=cfg.logging.get(\"project\", \"mlops-trilha-minima\"))\n",
        "    return CSVLogger(save_dir=\"logs\", name=\"runs\")\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    pl.seed_everything(seed, workers=True)\n",
        "\n",
        "@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"config\")\n",
        "def main(cfg: DictConfig):\n",
        "    print(\"Config:\\n\", OmegaConf.to_yaml(cfg))\n",
        "    set_seed(cfg.seed)\n",
        "\n",
        "    train_dl, val_dl, test_dl, tokenizer = get_dataloaders(\n",
        "        model_name=cfg.model.name,\n",
        "        dataset_name=cfg.data.dataset_name,\n",
        "        max_length=cfg.data.max_length,\n",
        "        batch_size=cfg.data.batch_size,\n",
        "    )\n",
        "\n",
        "    model = TextClassifier(\n",
        "        model_name=cfg.model.name,\n",
        "        num_labels=cfg.model.num_labels,\n",
        "        lr=cfg.model.lr,\n",
        "        weight_decay=cfg.model.weight_decay,\n",
        "    )\n",
        "\n",
        "    logger = maybe_wandb_logger(cfg)\n",
        "    ckpt = ModelCheckpoint(monitor=\"val_f1\", mode=\"max\", save_top_k=1, dirpath=\"artifacts\", filename=\"model\")\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=cfg.trainer.max_epochs,\n",
        "        devices=cfg.trainer.devices,\n",
        "        precision=cfg.trainer.precision,\n",
        "        logger=logger,\n",
        "        log_every_n_steps=cfg.trainer.log_every_n_steps,\n",
        "        callbacks=[ckpt],\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_dl, val_dl)\n",
        "    trainer.test(model, test_dl, ckpt_path=ckpt.best_model_path if ckpt.best_model_path else None)\n",
        "    print(f\"Best checkpoint: {ckpt.best_model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fcdc3b",
      "metadata": {
        "id": "90fcdc3b"
      },
      "source": [
        "### `export_onnx.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9030fc",
      "metadata": {
        "id": "3f9030fc"
      },
      "outputs": [],
      "source": [
        "%%writefile export_onnx.py\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "def export(model_dir=\"artifacts/hf\", out_path=\"artifacts/model.onnx\", opset=13):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    model.eval()\n",
        "\n",
        "    dummy = tokenizer(\"hello world\", return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    inputs = (dummy[\"input_ids\"], dummy[\"attention_mask\"])\n",
        "    dynamic_axes = {\"input_ids\": {0: \"batch\", 1: \"sequence\"}, \"attention_mask\": {0: \"batch\", 1: \"sequence\"}, \"logits\": {0: \"batch\"}}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            args=inputs,\n",
        "            f=out_path,\n",
        "            input_names=[\"input_ids\", \"attention_mask\"],\n",
        "            output_names=[\"logits\"],\n",
        "            dynamic_axes=dynamic_axes,\n",
        "            opset_version=opset,\n",
        "        )\n",
        "    print(f\"Exported ONNX to {out_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    export()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c50fa611",
      "metadata": {
        "id": "c50fa611"
      },
      "source": [
        "### `infer.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41d0458",
      "metadata": {
        "id": "e41d0458"
      },
      "outputs": [],
      "source": [
        "%%writefile infer.py\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(text: str, model_dir: str = \"artifacts/hf\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        out = model(**inputs).logits\n",
        "        probs = F.softmax(out, dim=-1).squeeze().tolist()\n",
        "        pred = int(out.argmax(dim=-1).item())\n",
        "    return {\"pred\": pred, \"probs\": probs}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--text\", type=str, required=True)\n",
        "    args = parser.parse_args()\n",
        "    print(predict(args.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a63d767",
      "metadata": {
        "id": "1a63d767"
      },
      "source": [
        "### `infer_onnx.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1bd9d1a",
      "metadata": {
        "id": "d1bd9d1a"
      },
      "outputs": [],
      "source": [
        "%%writefile infer_onnx.py\n",
        "import argparse\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from transformers import AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "def predict(text: str, model_path: str = \"artifacts/model.onnx\", tokenizer_dir: str = \"artifacts/hf\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
        "    sess = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
        "    enc = tokenizer(text, return_tensors=\"np\", truncation=True, max_length=128)\n",
        "    inputs = {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]}\n",
        "    logits = sess.run([\"logits\"], inputs)[0]\n",
        "    logits_t = torch.from_numpy(logits)\n",
        "    probs = F.softmax(logits_t, dim=-1).squeeze().tolist()\n",
        "    pred = int(np.argmax(logits, axis=-1).item())\n",
        "    return {\"pred\": pred, \"probs\": probs}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--text\", type=str, required=True)\n",
        "    args = parser.parse_args()\n",
        "    print(predict(args.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef1b203",
      "metadata": {
        "id": "8ef1b203"
      },
      "source": [
        "## 4) Treinamento (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4736acb0",
      "metadata": {
        "id": "4736acb0"
      },
      "outputs": [],
      "source": [
        "# Execute o treinamento conforme definido no seu script `train.py`.\n",
        "# Ajuste as flags/paths internos do script se necess√°rio.\n",
        "import os\n",
        "\n",
        "# Caso seu `train.py` use relative imports (model, data_utils), os arquivos j√° foram gravados acima.\n",
        "# Se o script tiver um bloco `if __name__ == \"__main__\":`, podemos cham√°-lo como m√≥dulo:\n",
        "\n",
        "import runpy\n",
        "env = os.environ.copy()\n",
        "env[\"MODELS_DIR\"] = str(MODELS_DIR)\n",
        "env[\"DATA_DIR\"] = str(DATA_DIR)\n",
        "# Executa o train.py como script\n",
        "runpy.run_path(\"train.py\", run_name=\"__main__\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b609bee",
      "metadata": {
        "id": "7b609bee"
      },
      "source": [
        "## 5) Exporta√ß√£o para ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8759265",
      "metadata": {
        "id": "d8759265"
      },
      "outputs": [],
      "source": [
        "# Exportar o modelo treinado para ONNX, usando o script `export_onnx.py`\n",
        "import os, runpy\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"MODELS_DIR\"] = str(MODELS_DIR)\n",
        "env[\"ONNX_DIR\"] = str(ONNX_DIR)\n",
        "\n",
        "runpy.run_path(\"export_onnx.py\", run_name=\"__main__\")\n",
        "print(\"\\nArquivos ONNX em:\", list(ONNX_DIR.glob(\"*.onnx\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "943f964f",
      "metadata": {
        "id": "943f964f"
      },
      "source": [
        "## 6) Infer√™ncia (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fbb46e",
      "metadata": {
        "id": "b3fbb46e"
      },
      "outputs": [],
      "source": [
        "# Executa infer√™ncia usando PyTorch (script `infer.py`)\n",
        "import os, runpy\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"MODELS_DIR\"] = str(MODELS_DIR)\n",
        "env[\"DATA_DIR\"] = str(DATA_DIR)\n",
        "\n",
        "runpy.run_path(\"infer.py\", run_name=\"__main__\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e9b583",
      "metadata": {
        "id": "e4e9b583"
      },
      "source": [
        "## 7) Infer√™ncia (ONNX Runtime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a0ef1da",
      "metadata": {
        "id": "6a0ef1da"
      },
      "outputs": [],
      "source": [
        "# Executa infer√™ncia usando ONNX Runtime (script `infer_onnx.py`)\n",
        "import os, runpy\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"ONNX_DIR\"] = str(ONNX_DIR)\n",
        "env[\"DATA_DIR\"] = str(DATA_DIR)\n",
        "\n",
        "runpy.run_path(\"infer_onnx.py\", run_name=\"__main__\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed29615",
      "metadata": {
        "id": "fed29615"
      },
      "source": [
        "\n",
        "## 8) Troubleshooting (erros comuns)\n",
        "- **ModuleNotFoundError**: confirme as instala√ß√µes na c√©lula de depend√™ncias; rode-a novamente.\n",
        "- **ImportError relativo**: como gravamos os m√≥dulos com os mesmos nomes, os imports devem funcionar. Se houver pacotes/paths personalizados no seu projeto, ajuste `sys.path` no in√≠cio do notebook.\n",
        "- **CUDA/vers√£o do Torch**: caso a GPU do Colab n√£o esteja ativa, o PyTorch instalar√° uma vers√£o CPU. Ative a GPU em *Runtime ‚Üí Change runtime type*.\n",
        "- **ONNX opset**: se seu export esperar um `opset_version` espec√≠fico, ajuste no `export_onnx.py`.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}