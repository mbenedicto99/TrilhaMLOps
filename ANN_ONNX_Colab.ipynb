{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbenedicto99/TrilhaMLOps/blob/main/ANN_ONNX_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f140806",
      "metadata": {
        "id": "6f140806"
      },
      "source": [
        "\n",
        "# 🔧 RNA em PyTorch + Exportação/Inferência ONNX — Notebook Colab\n",
        "Este notebook reúne e organiza os scripts fornecidos (`data_utils.py`, `model.py`, `train.py`, `export_onnx.py`, `infer.py`, `infer_onnx.py`) para rodar no Google Colab, com etapas claras de **setup**, **treino**, **exportação para ONNX** e **inferência (PyTorch e ONNX)**.\n",
        "\n",
        "> Dica: execute as células na ordem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66e304a",
      "metadata": {
        "id": "d66e304a"
      },
      "source": [
        "## 1) Ambiente e dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6bd5b43f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bd5b43f",
        "outputId": "744a00a5-722d-4909-f3a1-a9366cdf0cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "Plataforma: Linux-6.1.123+-x86_64-with-glibc2.35\n",
            "CUDA disponível: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Verificar versão do Python e GPU\n",
        "import sys, platform, torch\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Plataforma:\", platform.platform())\n",
        "print(\"CUDA disponível:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f1719a2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1719a2e",
        "outputId": "f73c0b95-dbc4-4ab9-c36b-a27004ad4c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'onnxsim' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'onnxsim'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependências (ajuste conforme necessário)\n",
        "# Obs.: Colab já traz muitas libs; caso falte algo do seu projeto, inclua aqui.\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install pytorch-lightning onnx onnxruntime onnxsim\n",
        "# onnxruntime-gpu é opcional; ativa se houver GPU compatível\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        !pip -q install onnxruntime-gpu\n",
        "except Exception as e:\n",
        "    print(\"Aviso: onnxruntime-gpu não instalado:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ee010d9",
      "metadata": {
        "id": "9ee010d9"
      },
      "source": [
        "## 2) Estrutura do projeto e caminhos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1cc1f96b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cc1f96b",
        "outputId": "aac8e885-ec16-4aa0-a681-a06bd0a0f823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROOT: /content\n",
            "DATA_DIR: /content/data\n",
            "MODELS_DIR: /content/models\n",
            "ONNX_DIR: /content/onnx\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "MODELS_DIR = ROOT / \"models\"\n",
        "ONNX_DIR = ROOT / \"onnx\"\n",
        "\n",
        "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
        "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "ONNX_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"MODELS_DIR:\", MODELS_DIR)\n",
        "print(\"ONNX_DIR:\", ONNX_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6f4a8f",
      "metadata": {
        "id": "3c6f4a8f"
      },
      "source": [
        "### (Opcional) Montar Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21957e35",
      "metadata": {
        "id": "21957e35"
      },
      "outputs": [],
      "source": [
        "USE_GDRIVE = False  # mude para True se quiser salvar no seu Drive\n",
        "if USE_GDRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Ajuste os diretórios, se desejar salvar no Drive:\n",
        "    # ROOT = Path('/content/drive/MyDrive/seu_projeto')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbea5957",
      "metadata": {
        "id": "bbea5957"
      },
      "source": [
        "## 3) Código-fonte — módulos do projeto\n",
        "Abaixo estão os conteúdos dos arquivos originais gravados como módulos locais para facilitar os imports."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98389b01",
      "metadata": {
        "id": "98389b01"
      },
      "source": [
        "### `data_utils.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "288d11ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "288d11ba",
        "outputId": "0620cce2-36a8-4c2e-97bd-ebc7c021ee5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data_utils.py\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_dataloaders(model_name: str, dataset_name: str = \"ag_news\", max_length: int = 128, batch_size: int = 16):\n",
        "    ds = load_dataset(dataset_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch[\"text\"], truncation=True, max_length=max_length)\n",
        "\n",
        "    tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "    tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "    tokenized.set_format(type=\"torch\")\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    train_dl = DataLoader(tokenized[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
        "    test_dl = DataLoader(tokenized[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    # Criar validação simples a partir do train (pequeno split)\n",
        "    val_size = min(4000, len(tokenized[\"train\"]))\n",
        "    val_subset = torch.utils.data.Subset(tokenized[\"train\"], range(val_size))\n",
        "    val_dl = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    return train_dl, val_dl, test_dl, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e956d9f3",
      "metadata": {
        "id": "e956d9f3"
      },
      "source": [
        "### `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "147a9453",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "147a9453",
        "outputId": "59611505-8293-457c-c755-078b65a8290d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "from typing import Any, Dict\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
        "\n",
        "class TextClassifier(pl.LightningModule):\n",
        "    def __init__(self, model_name: str, num_labels: int = 4, lr: float = 5e-5, weight_decay: float = 0.01):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "        self.acc = MulticlassAccuracy(num_classes=num_labels, average=\"macro\")\n",
        "        self.f1 = MulticlassF1Score(num_classes=num_labels, average=\"macro\")\n",
        "\n",
        "    def forward(self, **batch):\n",
        "        return self.model(**batch)\n",
        "\n",
        "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
        "        out = self(**batch)\n",
        "        preds = out.logits.argmax(dim=-1)\n",
        "        acc = self.acc(preds, batch[\"labels\"])\n",
        "        f1 = self.f1(preds, batch[\"labels\"])\n",
        "        self.log_dict({\"train_loss\": out.loss, \"train_acc\": acc, \"train_f1\": f1}, prog_bar=True, on_step=True, on_epoch=True)\n",
        "        return out.loss\n",
        "\n",
        "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
        "        out = self(**batch)\n",
        "        preds = out.logits.argmax(dim=-1)\n",
        "        acc = self.acc(preds, batch[\"labels\"])\n",
        "        f1 = self.f1(preds, batch[\"labels\"])\n",
        "        self.log_dict({\"val_loss\": out.loss, \"val_acc\": acc, \"val_f1\": f1}, prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
        "        out = self(**batch)\n",
        "        preds = out.logits.argmax(dim=-1)\n",
        "        acc = self.acc(preds, batch[\"labels\"])\n",
        "        f1 = self.f1(preds, batch[\"labels\"])\n",
        "        self.log_dict({\"test_loss\": out.loss, \"test_acc\": acc, \"test_f1\": f1}, prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
        "\n",
        "    def on_train_end(self) -> None:\n",
        "        # Salva pesos HF finetunados para export ONNX posterior\n",
        "        self.model.save_pretrained(\"artifacts/hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff4584ec",
      "metadata": {
        "id": "ff4584ec"
      },
      "source": [
        "### `train.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "731349ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "731349ae",
        "outputId": "76ba1bbc-7f54-4aee-ca50-e66bc41a576b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import hydra\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "\n",
        "from src.data_utils import get_dataloaders\n",
        "from src.model import TextClassifier\n",
        "\n",
        "def maybe_wandb_logger(cfg):\n",
        "    use_wb = bool(cfg.logging.get(\"use_wandb\", False))\n",
        "    if use_wb and os.environ.get(\"WANDB_API_KEY\"):\n",
        "        import wandb\n",
        "        from pytorch_lightning.loggers import WandbLogger\n",
        "        wandb.login()\n",
        "        return WandbLogger(project=cfg.logging.get(\"project\", \"mlops-trilha-minima\"))\n",
        "    return CSVLogger(save_dir=\"logs\", name=\"runs\")\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    pl.seed_everything(seed, workers=True)\n",
        "\n",
        "@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"config\")\n",
        "def main(cfg: DictConfig):\n",
        "    print(\"Config:\\n\", OmegaConf.to_yaml(cfg))\n",
        "    set_seed(cfg.seed)\n",
        "\n",
        "    train_dl, val_dl, test_dl, tokenizer = get_dataloaders(\n",
        "        model_name=cfg.model.name,\n",
        "        dataset_name=cfg.data.dataset_name,\n",
        "        max_length=cfg.data.max_length,\n",
        "        batch_size=cfg.data.batch_size,\n",
        "    )\n",
        "\n",
        "    model = TextClassifier(\n",
        "        model_name=cfg.model.name,\n",
        "        num_labels=cfg.model.num_labels,\n",
        "        lr=cfg.model.lr,\n",
        "        weight_decay=cfg.model.weight_decay,\n",
        "    )\n",
        "\n",
        "    logger = maybe_wandb_logger(cfg)\n",
        "    ckpt = ModelCheckpoint(monitor=\"val_f1\", mode=\"max\", save_top_k=1, dirpath=\"artifacts\", filename=\"model\")\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=cfg.trainer.max_epochs,\n",
        "        devices=cfg.trainer.devices,\n",
        "        precision=cfg.trainer.precision,\n",
        "        logger=logger,\n",
        "        log_every_n_steps=cfg.trainer.log_every_n_steps,\n",
        "        callbacks=[ckpt],\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_dl, val_dl)\n",
        "    trainer.test(model, test_dl, ckpt_path=ckpt.best_model_path if ckpt.best_model_path else None)\n",
        "    print(f\"Best checkpoint: {ckpt.best_model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fcdc3b",
      "metadata": {
        "id": "90fcdc3b"
      },
      "source": [
        "### `export_onnx.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3f9030fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f9030fc",
        "outputId": "0931da30-1dba-446c-fabb-95e190a6fbe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing export_onnx.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile export_onnx.py\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "def export(model_dir=\"artifacts/hf\", out_path=\"artifacts/model.onnx\", opset=13):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    model.eval()\n",
        "\n",
        "    dummy = tokenizer(\"hello world\", return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    inputs = (dummy[\"input_ids\"], dummy[\"attention_mask\"])\n",
        "    dynamic_axes = {\"input_ids\": {0: \"batch\", 1: \"sequence\"}, \"attention_mask\": {0: \"batch\", 1: \"sequence\"}, \"logits\": {0: \"batch\"}}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            args=inputs,\n",
        "            f=out_path,\n",
        "            input_names=[\"input_ids\", \"attention_mask\"],\n",
        "            output_names=[\"logits\"],\n",
        "            dynamic_axes=dynamic_axes,\n",
        "            opset_version=opset,\n",
        "        )\n",
        "    print(f\"Exported ONNX to {out_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    export()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c50fa611",
      "metadata": {
        "id": "c50fa611"
      },
      "source": [
        "### `infer.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e41d0458",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e41d0458",
        "outputId": "c826fd7a-680b-42b4-85b9-591dbb92199c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing infer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile infer.py\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(text: str, model_dir: str = \"artifacts/hf\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        out = model(**inputs).logits\n",
        "        probs = F.softmax(out, dim=-1).squeeze().tolist()\n",
        "        pred = int(out.argmax(dim=-1).item())\n",
        "    return {\"pred\": pred, \"probs\": probs}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--text\", type=str, required=True)\n",
        "    args = parser.parse_args()\n",
        "    print(predict(args.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a63d767",
      "metadata": {
        "id": "1a63d767"
      },
      "source": [
        "### `infer_onnx.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d1bd9d1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1bd9d1a",
        "outputId": "ec871434-e534-4624-ff6b-df17a9cb8eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing infer_onnx.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile infer_onnx.py\n",
        "import argparse\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from transformers import AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "def predict(text: str, model_path: str = \"artifacts/model.onnx\", tokenizer_dir: str = \"artifacts/hf\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
        "    sess = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
        "    enc = tokenizer(text, return_tensors=\"np\", truncation=True, max_length=128)\n",
        "    inputs = {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]}\n",
        "    logits = sess.run([\"logits\"], inputs)[0]\n",
        "    logits_t = torch.from_numpy(logits)\n",
        "    probs = F.softmax(logits_t, dim=-1).squeeze().tolist()\n",
        "    pred = int(np.argmax(logits, axis=-1).item())\n",
        "    return {\"pred\": pred, \"probs\": probs}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--text\", type=str, required=True)\n",
        "    args = parser.parse_args()\n",
        "    print(predict(args.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef1b203",
      "metadata": {
        "id": "8ef1b203"
      },
      "source": [
        "## 4) Treinamento (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hydra-core omegaconf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUMIbR-lxoFy",
        "outputId": "cc385895-a2e7-4455-b163-b86b64984a28"
      },
      "id": "CUMIbR-lxoFy",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core) (25.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf) (6.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Installing collected packages: hydra-core\n",
            "Successfully installed hydra-core-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4736acb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4736acb0",
        "outputId": "b8aaa3f1-f539-4a73-ab84-aa1c54562893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROOT      : /content\n",
            "SRC       : /content/src\n",
            "CONFIGS   : /content/configs\n",
            "DATA_DIR  : /content/data\n",
            "MODELS_DIR: /content/models\n",
            "\n",
            "> Executando: /usr/bin/python3 -m src.train hydra.run.dir=. hydra.output_subdir=null \n",
            "\n",
            "\n",
            "✅ Treinamento finalizado com sucesso.\n"
          ]
        }
      ],
      "source": [
        "# ==== Treino isolado (Hydra) — robusto para Colab ====\n",
        "import os, sys, subprocess, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "SRC  = ROOT / \"src\"\n",
        "CONF = ROOT / \"configs\"\n",
        "MODELS_DIR = ROOT / \"models\"\n",
        "DATA_DIR   = ROOT / \"data\"\n",
        "\n",
        "# 0) Dependências necessárias\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                \"pytorch-lightning\", \"omegaconf\", \"hydra-core\",\n",
        "                \"transformers\", \"datasets\", \"torchmetrics\"], check=False)\n",
        "\n",
        "# 1) Estrutura de pacotes\n",
        "SRC.mkdir(exist_ok=True)\n",
        "(MODELS_DIR).mkdir(exist_ok=True)\n",
        "(DATA_DIR).mkdir(exist_ok=True)\n",
        "(SRC / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
        "\n",
        "# Copia módulos para o pacote src/\n",
        "def cp(src_name, dst_rel):\n",
        "    if Path(src_name).exists():\n",
        "        shutil.copyfile(src_name, SRC / dst_rel)\n",
        "\n",
        "cp(\"data_utils.py\", \"data_utils.py\")\n",
        "cp(\"model.py\", \"model.py\")\n",
        "\n",
        "# Mover/copiar o train.py para dentro de src/ (para que config_path ../configs => /content/configs)\n",
        "if Path(\"train.py\").exists():\n",
        "    shutil.copyfile(\"train.py\", SRC / \"train.py\")\n",
        "\n",
        "# 2) Config Hydra (o decorator aponta para ../configs)\n",
        "CONF.mkdir(exist_ok=True)\n",
        "cfg_path = CONF / \"config.yaml\"\n",
        "if not cfg_path.exists():\n",
        "    cfg_path.write_text(\n",
        "        \"\"\"# Config mínima para o seu script\n",
        "seed: 42\n",
        "\n",
        "logging:\n",
        "  use_wandb: false\n",
        "\n",
        "data:\n",
        "  dataset_name: ag_news\n",
        "  max_length: 128\n",
        "  batch_size: 16\n",
        "\n",
        "model:\n",
        "  name: distilbert-base-uncased\n",
        "  num_labels: 4\n",
        "  lr: 5e-5\n",
        "  weight_decay: 0.01\n",
        "\n",
        "trainer:\n",
        "  max_epochs: 1          # aumente depois\n",
        "  devices: 1\n",
        "  precision: 32\n",
        "  log_every_n_steps: 10\n",
        "\"\"\",\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "# 3) Ambiente\n",
        "os.environ[\"PYTHONPATH\"] = f\"{str(ROOT)}\" + (\":\" + os.environ[\"PYTHONPATH\"] if \"PYTHONPATH\" in os.environ else \"\")\n",
        "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
        "\n",
        "print(\"ROOT      :\", ROOT)\n",
        "print(\"SRC       :\", SRC)\n",
        "print(\"CONFIGS   :\", CONF)\n",
        "print(\"DATA_DIR  :\", DATA_DIR)\n",
        "print(\"MODELS_DIR:\", MODELS_DIR)\n",
        "\n",
        "# 4) Execução isolada (sem herdar os -f do Jupyter)\n",
        "#    Mantemos o diretório de execução e desativamos a subpasta .hydra\n",
        "cmd = [sys.executable, \"-m\", \"src.train\", \"hydra.run.dir=.\", \"hydra.output_subdir=null\"]\n",
        "\n",
        "print(\"\\n> Executando:\", \" \".join(cmd), \"\\n\")\n",
        "try:\n",
        "    subprocess.run(cmd, check=True)\n",
        "    print(\"\\n✅ Treinamento finalizado com sucesso.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    # Se falhar, roda capturando stdout/err e exibe tudo\n",
        "    print(\"❌ Falha no treinamento. Reexecutando para capturar logs...\\n\")\n",
        "    r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(r.stdout)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b609bee",
      "metadata": {
        "id": "7b609bee"
      },
      "source": [
        "## 5) Exportação para ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8759265",
      "metadata": {
        "id": "d8759265"
      },
      "outputs": [],
      "source": [
        "# Exportar o modelo treinado para ONNX, usando o script `export_onnx.py`\n",
        "import os, runpy\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"MODELS_DIR\"] = str(MODELS_DIR)\n",
        "env[\"ONNX_DIR\"] = str(ONNX_DIR)\n",
        "\n",
        "runpy.run_path(\"export_onnx.py\", run_name=\"__main__\")\n",
        "print(\"\\nArquivos ONNX em:\", list(ONNX_DIR.glob(\"*.onnx\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "943f964f",
      "metadata": {
        "id": "943f964f"
      },
      "source": [
        "## 6) Inferência (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fbb46e",
      "metadata": {
        "id": "b3fbb46e"
      },
      "outputs": [],
      "source": [
        "# Executa inferência usando PyTorch (script `infer.py`)\n",
        "import os, runpy\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"MODELS_DIR\"] = str(MODELS_DIR)\n",
        "env[\"DATA_DIR\"] = str(DATA_DIR)\n",
        "\n",
        "runpy.run_path(\"infer.py\", run_name=\"__main__\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e9b583",
      "metadata": {
        "id": "e4e9b583"
      },
      "source": [
        "## 7) Inferência (ONNX Runtime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a0ef1da",
      "metadata": {
        "id": "6a0ef1da"
      },
      "outputs": [],
      "source": [
        "# Executa inferência usando ONNX Runtime (script `infer_onnx.py`)\n",
        "import os, runpy\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"ONNX_DIR\"] = str(ONNX_DIR)\n",
        "env[\"DATA_DIR\"] = str(DATA_DIR)\n",
        "\n",
        "runpy.run_path(\"infer_onnx.py\", run_name=\"__main__\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed29615",
      "metadata": {
        "id": "fed29615"
      },
      "source": [
        "\n",
        "## 8) Troubleshooting (erros comuns)\n",
        "- **ModuleNotFoundError**: confirme as instalações na célula de dependências; rode-a novamente.\n",
        "- **ImportError relativo**: como gravamos os módulos com os mesmos nomes, os imports devem funcionar. Se houver pacotes/paths personalizados no seu projeto, ajuste `sys.path` no início do notebook.\n",
        "- **CUDA/versão do Torch**: caso a GPU do Colab não esteja ativa, o PyTorch instalará uma versão CPU. Ative a GPU em *Runtime → Change runtime type*.\n",
        "- **ONNX opset**: se seu export esperar um `opset_version` específico, ajuste no `export_onnx.py`.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}